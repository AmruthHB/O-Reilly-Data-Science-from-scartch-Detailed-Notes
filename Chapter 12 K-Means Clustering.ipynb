{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 13: K-nearest neigbours\n",
    "\n",
    "#### Idea\n",
    "- Consider that you have a person. You know this person's neigbours information including: their location, their income, the amount of kids they have, age.\n",
    "- From the data of neigbours, it could be extrapolated to the individual in question as they too should have similar information, in some aspects like their neigbours.\n",
    "- Looking at close pieces of data and extrapolating is main idea of KNN\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Model\n",
    "- Simple predictive model. Requires:\n",
    "    - A notion of distance\n",
    "    - An assumption that points close to one another are similar\n",
    "- Unlike many models which consider entire data, KNN only considers data points in closest proximity, not entire dataset.\n",
    "- Has low explainability: does not explain why your neigbour voting for something makes you vote for the same thing (you may have different reasons), other models might indicate that your martial status and kids impact this.\n",
    "<br>\n",
    "- Ideally we have data and labels (T\\F, discrete options)\n",
    "- Here data points are vectors (Numeric Values)\n",
    "- k: the number of closest points you will use to extrapolate your new data point\n",
    "- We choose k closest values and make them vote on the output\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "##### Part 1: Helpers\n",
    "\n",
    "1. Counting most popular values from an array of values\n",
    "    - make the array into Counter dict\n",
    "    - .most common on counter structure\n",
    "2. Tie breaker: what if two values have same # of votes? We can do the following:\n",
    "    - Pick random winner\n",
    "    - Weight votes by distance and pick winner\n",
    "    - Reduce k till we find winner\n",
    "    \n",
    "- Ideally, we reduce k till we find winner. So essentially, you cut out last value from array and rerun counter till tie is broken or one value exists in counter structure.\n",
    "\n",
    "##### Algorithm\n",
    "\n",
    "```python\n",
    "from typing import NamedTuple\n",
    "from collections import Counter\n",
    "from scratch.linear_algebra import Vector, distance\n",
    "\n",
    "class LabelledPoint(NamedTuple):\n",
    "    Point: Vector\n",
    "    label: str\n",
    "\n",
    "        \n",
    "def most_voted(neigbours: List[str]) -> str:\n",
    "    votes = Counter(neigbours)\n",
    "    winner, most_count = votes.most_counts(1)[0]\n",
    "    ties = len([ label for label in votes if votes[label] == most_count])\n",
    "    if ties == 1:\n",
    "        return winner\n",
    "    else:\n",
    "        most_voted(neigbours[:-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "def knn_classify(k: int,\n",
    "                 labelled_points: List[LabelledPoint],\n",
    "                 new_point: Vector) -> str:\n",
    "    # order labelled points by distance (ascending sort)\n",
    "    by_distance = sorted(labelled_points, key = lambda lp: distance(lp.point,new_point))\n",
    "    \n",
    "    # snatch k nearest labels\n",
    "    k_neigbours = [lp.label for lp in by_distance]\n",
    "    \n",
    "    # do voting (function mentioned in helper)\n",
    "    return most_voted(k_neigbours)\n",
    "                 \n",
    "                 \n",
    "                 \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "- K nearest is straightforward, no IRIS covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curse of Dimensionality\n",
    "\n",
    "- K means has issues with high dimensional vectors due to the curse of dimensionality\n",
    "- Why? High dimensional spaces vast, points are rarely close together\n",
    "- In general, more dimensions, average distance between points increases\n",
    "\n",
    "- Bigger issue: difference between closest distance to a point and the average distances of the points of the dataset.\n",
    "- Low dimensional dataset, closest points closer than average. In high dimensional datasets, for a given point the closest points are very close to the average. This would lead to greater inaccuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
