{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11: Machine Learning\n",
    "\n",
    "- What is data science?\n",
    "- Data science is not just making and tweaking ML models.\n",
    "- Data science is taking a business problem, deciding what type of data to collect to address that problem, collecting data, understanding what the data says using statistics, cleaning data, formatting data and then machine learning.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Modelling\n",
    "\n",
    "- Model: Mathematical/Probailistic releationship that exists between differnet variables.\n",
    "- A lot of the time models are fairly specific: \n",
    "    - Ex: input of different ingridients to make cookies\n",
    "    - Poker probabilities\n",
    "<br>\n",
    "\n",
    "#### Machine Learning\n",
    "- Are models that are learned from data\n",
    "- Use existsing data to construct a model such that we can predit future outcomes given inputs.\n",
    "    - Ex: spam mail or not, ad a shopper is most likely to click on\n",
    " \n",
    "- Types of ML models\n",
    "    - Supervised: Set of data with labelled correct answers to learn from\n",
    "    - Unsupervised: data but not labels\n",
    "    - Semi supervised (only some data is labelled)\n",
    "    - On-line: model adjusts to incoming data\n",
    "    - reinforcement learnig: model predict, learn how well it did.\n",
    "\n",
    "- There are an insane amount of models that can be used on a given set of data, it is up to us to make an informed prediction and make a model from that. Ex: we observe relationship between height and weight is linear, we hypothesize it is a linear regression.\n",
    "\n",
    "<br>\n",
    "\n",
    "####  Overfitting and Underfitting\n",
    "- dangers of ML\n",
    "- Overfitting: model that trains well on data, generalizes poorly to new data (perhaps model learns noise in data)\n",
    "\n",
    "- Undefitting: Model that dosen't perform well on training data. Usually means chosen model not good enough.\n",
    "\n",
    "- To avoid underfitting/overfitting : Split into train and test datasets\n",
    "- This would mean that if our model performs poorly on train, it would likely perform poorly on test, vice versa.\n",
    "\n",
    "- Issue: common patterns exist in test/train dataset that don't generalize to larger datasets.\n",
    "\n",
    "- Even bigger issue: choosing train/test to choose from several models. If you use test set as a metric and choose model that performs best on test set as the desired model it is called \"meta training\" AVOID THIS.\n",
    "\n",
    "- Solution: have three sets\n",
    "    - Train: train all data on this\n",
    "    - Validation: check which model is the best from this set\n",
    "    - Test: Actually test model here \n",
    "<br>\n",
    "\n",
    "#### Correctness\n",
    "- Labelling is not binary. We have 4 types of results\n",
    "    - True positive (tp) : \"Message is spam, we correctly classified it as such\"\n",
    "    - False positive (fp) (type 1 error) : \"Message is not spam, we classified it as spam\"\n",
    "    - False negative (fn) (type 2 error): \"Message is spam, we classified it as not-spam\"\n",
    "    - True negative (tn): \"Message is not-spam, we classified it as not-spam\"\n",
    "- Called confusion matrix\n",
    "- We can use this to derive model statistics. \n",
    "\n",
    "<br> \n",
    "\n",
    "1. Accuracy: Fraction of correct predictions (everything we got correct out of all predictions)\n",
    "$$  Accuracy = \\frac{tp + tn}{tp + fp + fn + tn} $$\n",
    "<br> \n",
    "2. Precision: Accuracy of positive predictions (from all our positives, how many were correct)\n",
    "$$ Precision = \\frac{tp}{tp + fp} $$\n",
    "<br>\n",
    "3. Recall: Measures what fraction of positives our model identified . tp+fn is everything that should have been positive, tp is what we labelled correctly.\n",
    "$$ Recall = \\frac{tp}{tp + fn} $$\n",
    "<br>\n",
    "4. F1 score: combines precision and recall is also called their harmonic mean and usually lies between them.\n",
    "$$ f1 score = \\frac{2 * precision * recall}{precision + recall} $$\n",
    "<br>\n",
    "\n",
    "\n",
    "- Note: Accuracy isn't the greatest measure.\n",
    "\n",
    "##### Precision Recall trade-off\n",
    "- model that predicts \"yes\" with little confidence usually has high recall, low precision\n",
    "- model that predicts \"yes\" only if it has extreme confidence: low recall, high precision\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Bias Variance tradeoff\n",
    "\n",
    "- Another aspect of overfitting vs underfitting problem\n",
    "- Consider a model that is severly underfitted:\n",
    "- It has high bias: lots of mistakes on training set\n",
    "- If we rpelaced our training set with a similar trianing set, the model woudn't change much: low variance.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Consider a model that is severly overfitted:\n",
    "- It has low bias: barely any mistakes on training set\n",
    "- High variance: if we slightly chnage to new traing set, model fails very badly\n",
    "\n",
    "\n",
    "Analyzing bias, variance allows us to imporve models.\n",
    "- high bias: usually add more features\n",
    "- high variance: remove features or get more data\n",
    "\n",
    "\n",
    "#### Feature Extraction and selection\n",
    "\n",
    "- Too little features: easy to underfit\n",
    "- Too many features: easy to overfit\n",
    "- Best case: features given to you (ex: want to predict salary vs experience). In case model does not linearly fit well, we could do some by-hand tricks such as experience squared vs salary , experience cubed vs salary.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Typically more complicated data, use off hand tricks to manually determine features to extract. \n",
    "Ex, given raw text determine the following:\n",
    "- What was the domain of the sender\n",
    "- was a there a mention of free and gift card?\n",
    "\n",
    "<br>\n",
    "- Thus, our features could be yes/no , numbers or a set of discrete options.\n",
    "- Features also determine models we can use:\n",
    "    - Naive Bayes:  for yes/no\n",
    "    - Regression: numeric features\n",
    "    - decision trees: categorical/numerical data\n",
    "<br>\n",
    "\n",
    "- Sometimes we have too many features, we have to:\n",
    "    - do dimenisonal reduction\n",
    "    - regualarization: penalize model for more features used.\n",
    "- Feature selection usually comes with experience, no obvious formula \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
