{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9: Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notes aren't detailed because scraping is more reading the docs than knowledge-based\n",
    "\n",
    "#### stdin and stdout\n",
    "- can pipe data through command line using sys.stdin and sys.stdout\n",
    "- Personal note: make use of regex to get information about files when processing them\n",
    "\n",
    "#### Reading Files\n",
    "##### Text Files\n",
    "- open function useful for reading files\n",
    "- note that opened files must always be closed\n",
    "```python\n",
    "file = open('ex.txt','r')\n",
    "file.close()       \n",
    "```\n",
    "- read r is an assumed default parameter, changing r to w, write would overwrite file\n",
    "- a would open the file for appending and create it if it does not exist already\n",
    "\n",
    "- Alternative to closing is using a with block\n",
    "```python\n",
    "with open(file) as f:\n",
    "    data = extract_function(f)\n",
    "# process\n",
    "```\n",
    "- Can also use for to go through each line of text file\n",
    "- As well as that can use regex, defaultdict and dict to get data/info about data\n",
    "- .strip() is a useful function for removing \\n when reading text files\n",
    "- Example: .txt full of mail adresses, we want to construct a histogram of all domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'gmail.com': 1, 'buycat.com': 1, 'buydog.com': 1, 'pmail.com': 1})\n",
      "['bob@gmail.com\\n', 'chester@buycat.com\\n', 'manny@buydog.com\\n', 'bobby@pmail.com\\n']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def get_domain(mail: str) -> str:\n",
    "    # split string at @ and get last portion of array\n",
    "    return mail.lower().split(\"@\")[-1] \n",
    "assert get_domain('banana@gmail.com') == 'gmail.com'\n",
    "\n",
    "from collections import Counter\n",
    "with open(os.getcwd()+'\\email_adresses.txt','r') as f:\n",
    "    domain_counts = Counter(get_domain(line.strip())\n",
    "                           for line in f\n",
    "                           if \"@\" in line)\n",
    "    \n",
    "print(f\"{domain_counts}\")\n",
    "\n",
    "# if we didnt strip each line we would have \\n at each end, which is whitespace\n",
    "with open(os.getcwd()+'\\email_adresses.txt','r') as f:\n",
    "    z = [l for l in f]\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Skip Delimited files because we have pandas\n",
    "\n",
    "#### Scraping The Web\n",
    "- Beautiful Soup for scraping\n",
    "- Requests Library for http\n",
    "- To use soup, pass string with html/link, then get text from it\n",
    "\n",
    "```python\n",
    "url = \"https://sekiroshadowsdietwice.wiki.fextralife.com/Items\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html,'html5lib')\n",
    "\n",
    "```\n",
    "- use soup.find('tag_name') to find tags and get data\n",
    "- extract text: soup.find('tag_name').text\n",
    "- get attributes like id, class etc: soup.p['id']\n",
    "- Find all tags soup.find_all('tag_name')\n",
    "- finding tags with specific class and id: [p for p in soup('p') if p.get('id')]\n",
    "\n",
    "```python\n",
    "# class in specific tags\n",
    "important_paragraphs = soup('p', {'class' : 'important'})\n",
    "important_paragraphs2 = soup('p', 'important')\n",
    "important_paragraphs3 = [p for p in soup('p')\n",
    "if 'important' in p.get('class', [])]\n",
    "\n",
    "# tag in tag with nested for\n",
    "spans_inside_divs = [span\n",
    "    for div in soup('div') \n",
    "    for span in div('span')]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Item                                        Description\n",
      "0  Ako's Spiritfall  Increases Vitality and Posture damage for a ti...\n",
      "1       Ako's Sugar  Temporarily boosts Vitality and Posture damage...\n",
      "2   Antidote Powder  Heals the status abnormality \"Poison\", reduces...\n",
      "3        Bell Demon  Possessing this item increases enemy difficult...\n",
      "4         Bite Down  The user dies instantly, but can resurrect if ...\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = \"https://sekiroshadowsdietwice.wiki.fextralife.com/Items\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html,'html5lib')\n",
    "table = soup.find(\"tbody\")\n",
    "table_data = [td.text for td in table(\"tr\")]\n",
    "\n",
    "import re\n",
    "# we want to store data as a dict where keys are the items and the values are effects\n",
    "def get_details(string):\n",
    "    return [item.strip() for item in (string.split(\"\\n\")) if re.search('[0-9a-zA-Z]',item)]\n",
    "\n",
    "final_data = {}\n",
    "for obj in table_data:\n",
    "    name,effect = get_details(obj)\n",
    "    final_data[name] = effect\n",
    "    \n",
    "\n",
    "# we have an ugly dict so now we will transfer it to pandas\n",
    "structured_data = {\"Item\": list(final_data.keys()),\"Description\": list(final_data.values())}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(structured_data)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Congress scraping example v.good </b> \n",
    "- Bottom line is regex and, filtering lists, keyword match (keyword.lower()) are best friends here.\n",
    "\n",
    "#### API\n",
    "- API: Application Programming Interface\n",
    "\n",
    "##### JSON & XML\n",
    "\n",
    "###### JSON\n",
    "- http protocol for text transfer, data requested tranformed into string format, serialization in the form of JSON\n",
    "- example json (very similar to dict)\n",
    "\n",
    "```python\n",
    "{ \"title\" : \"Data Science Book\",\n",
    "\"author\" : \"Joel Grus\",\n",
    "\"publicationYear\" : 2019,\n",
    "\"topics\" : [ \"data\", \"science\", \"data science\"] }\n",
    "```\n",
    "- json object can easily be transformed into dict, we just need to deserialize it from json into a python object\n",
    "\n",
    "```python\n",
    "import json\n",
    "desrialize = json.loads(serialized)\n",
    "\n",
    "# where serialized is something you call straight from https request\n",
    "```\n",
    "\n",
    "###### XML\n",
    "- XML is like html, can use beautifulsoup\n",
    "\n",
    "#### No to API\n",
    "- DOCS COVER THEM ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
